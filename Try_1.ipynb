{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_format(sec):\n",
    "    return str(timedelta(seconds=sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_raw(churned_start_date='2019-01-01', \n",
    "                      churned_end_date='2019-02-01', \n",
    "                      inter_list=[(1,7),(8,14)],\n",
    "                      raw_data_path='train/',\n",
    "                      dataset_path='dataset/', \n",
    "                      mode='train'):\n",
    "    \n",
    "    start_t = time.time()\n",
    " \n",
    "    sample = pd.read_csv('{}sample.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    profiles = pd.read_csv('{}profiles.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    payments = pd.read_csv('{}payments.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    reports = pd.read_csv('{}reports.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    abusers = pd.read_csv('{}abusers.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    logins = pd.read_csv('{}logins.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    pings = pd.read_csv('{}pings.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    sessions = pd.read_csv('{}sessions.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    shop = pd.read_csv('{}shop.csv'.format(raw_data_path), sep=';', na_values=['\\\\N', 'None'], encoding='utf-8')\n",
    "    \n",
    "    print('Run time (reading csv files): {}'.format(time_format(time.time()-start_t)))    \n",
    "#-----------------------------------------------------------------------------------------------------    \n",
    "    print('NO dealing with outliers, missing values and categorical features...')\n",
    "#-----------------------------------------------------------------------------------------------------        \n",
    "    # На основании дня отвала (last_login_dt) строим признаки, которые описывают активность игрока перед уходом\n",
    "    \n",
    "    print('Creating dataset...')\n",
    "    # Создадим пустой датасет - в зависимости от режима построения датасета - train или test\n",
    "    if mode == 'train':\n",
    "        dataset = sample.copy()[['user_id', 'is_churned', 'level', 'donate_total']]\n",
    "    elif mode == 'test':\n",
    "        dataset = sample.copy()[['user_id', 'level', 'donate_total']]\n",
    "\n",
    "    # Пройдемся по всем источникам, содержащим \"динамичекие\" данные\n",
    "    for df in [payments, reports, abusers, logins, pings, sessions, shop]:\n",
    "\n",
    "        # Получим 'day_num_before_churn' для каждого из значений в источнике для определения недели\n",
    "        data = pd.merge(sample[['user_id', 'login_last_dt']], df, on='user_id')\n",
    "        data['day_num_before_churn'] = 1 + (data['login_last_dt'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d')) - \n",
    "                                data['log_dt'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))).apply(lambda x: x.days)\n",
    "        df_features = data[['user_id']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        # Для каждого признака создадим признаки для каждого из времененно интервала (в нашем примере 4 интервала по 7 дней)\n",
    "        features = list(set(data.columns) - set(['user_id', 'login_last_dt', 'log_dt', 'day_num_before_churn']))\n",
    "        print('Processing with features:', features)\n",
    "        for feature in features:\n",
    "            for i, inter in enumerate(inter_list):\n",
    "                inter_df = data.loc[data['day_num_before_churn'].between(inter[0], inter[1], inclusive=True)].\\\n",
    "                                groupby('user_id')[feature].mean().reset_index().\\\n",
    "                                rename(index=str, columns={feature: feature+'_{}'.format(i+1)})\n",
    "                df_features = pd.merge(df_features, inter_df, how='left', on='user_id')\n",
    "\n",
    "        # Добавляем построенные признаки в датасет\n",
    "        dataset = pd.merge(dataset, df_features, how='left', on='user_id')\n",
    "        \n",
    "        print('Run time (calculating features): {}'.format(time_format(time.time()-start_t)))\n",
    "\n",
    "    # Добавляем \"статические\" признаки\n",
    "    dataset = pd.merge(dataset, profiles, on='user_id')\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "    dataset.to_csv('{}dataset_raw_{}.csv'.format(dataset_path, mode), sep=';', index=False)\n",
    "    print('Dataset is successfully built and saved to {}, run time \"build_dataset_raw\": {}'.\\\n",
    "          format(dataset_path, time_format(time.time()-start_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Следует из исходных данных\n",
    "CHURNED_START_DATE = '2019-09-01' \n",
    "CHURNED_END_DATE = '2019-10-01'\n",
    "\n",
    "INTER_1 = (1,7)\n",
    "INTER_2 = (8,14)\n",
    "INTER_3 = (15,21)\n",
    "INTER_4 = (22,28)\n",
    "INTER_LIST = [INTER_1, INTER_2, INTER_3, INTER_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dataset_raw(churned_start_date=CHURNED_START_DATE,\n",
    "                  churned_end_date=CHURNED_END_DATE,\n",
    "                  inter_list=INTER_LIST,\n",
    "                  raw_data_path='Data/train/',\n",
    "                  dataset_path='Data/dataset/', \n",
    "                  mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dataset_raw(churned_start_date=CHURNED_START_DATE,\n",
    "                  churned_end_date=CHURNED_END_DATE,\n",
    "                  inter_list=INTER_LIST,\n",
    "                  raw_data_path='Data/test/',\n",
    "                  dataset_path='Data/dataset/', \n",
    "                  mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/dataset/dataset_raw_train.csv', sep=';')\n",
    "test = pd.read_csv('Data/dataset/dataset_raw_test.csv', sep=';')\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, \n",
    "                    dataset_type='train',\n",
    "                    dataset_path='Data/dataset/'):\n",
    "    print(dataset_type)\n",
    "    start_t = time.time()\n",
    "    print('Dealing with missing values, outliers, categorical features...')\n",
    "    \n",
    "    # Профили\n",
    "    dataset['age'] = dataset['age'].fillna(dataset['age'].median())\n",
    "    dataset['gender'] = dataset['gender'].fillna(dataset['gender'].mode()[0])\n",
    "    dataset.loc[~dataset['gender'].isin(['M', 'F']), 'gender'] = dataset['gender'].mode()[0]\n",
    "    dataset['gender'] = dataset['gender'].map({'M': 1., 'F':0.})\n",
    "    dataset.loc[(dataset['age'] > 80) | (dataset['age'] < 7), 'age'] = round(dataset['age'].median())\n",
    "    dataset.loc[dataset['days_between_fl_df'] < -1, 'days_between_fl_df'] = -1\n",
    "    # Пинги\n",
    "    for period in range(1,len(INTER_LIST)+1):\n",
    "        col = 'avg_min_ping_{}'.format(period)\n",
    "        dataset.loc[(dataset[col] < 0) | \n",
    "                    (dataset[col].isnull()), col] = dataset.loc[dataset[col] >= 0][col].median()\n",
    "    # Сессии и прочее\n",
    "    dataset.fillna(0, inplace=True)\n",
    "    dataset.to_csv('{}dataset_{}.csv'.format(dataset_path, dataset_type), sep=';', index=False)\n",
    "         \n",
    "    print('Dataset is successfully prepared and saved to {}, run time (dealing with bad values): {}'.\\\n",
    "          format(dataset_path, time_format(time.time()-start_t))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dataset(dataset=train, dataset_type='train')\n",
    "prepare_dataset(dataset=test, dataset_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = pd.read_csv('Data/dataset/dataset_train.csv', sep=';')\n",
    "# test_new = pd.read_csv('dataset/dataset_test.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new['is_churned'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Балансировка классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras_preprocessing.image' has no attribute 'DataFrameIterator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-848a076e3dd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\imblearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mensemble\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mover_sampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\imblearn\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m in keras.\"\"\"\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_generator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBalancedBatchGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_generator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbalanced_batch_generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\imblearn\\keras\\_generator.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mParentClass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHAS_KERAS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_keras\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\imblearn\\keras\\_generator.py\u001b[0m in \u001b[0;36mimport_keras\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mParentClassKeras\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_keras_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_from_keras\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mParentClassTensorflow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_keras_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_from_tensforflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mhas_keras\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhas_keras_k\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mhas_keras_tf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\imblearn\\keras\\_generator.py\u001b[0m in \u001b[0;36mimport_from_keras\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mimport_from_keras\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\datasets\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreuters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\datasets\\imdb.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_remove_long_seq\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\preprocessing\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mDataFrameIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrameIterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrameIterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras_preprocessing.image' has no attribute 'DataFrameIterator'"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_new.drop(['user_id', 'is_churned'], axis=1)\n",
    "y_train = train_new['is_churned']\n",
    "\n",
    "X_train_mm = MinMaxScaler().fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train_balanced, y_train_balanced = SMOTE(random_state=42, sampling_strategy=0.3). \\\n",
    "                                        fit_sample(X_train_mm, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print('До:', Counter(y_train.values))\n",
    "print('После:', Counter(y_train_balanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python37064bitb4d39468b91d46529730ac7640a42948"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
